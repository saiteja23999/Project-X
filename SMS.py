import socket
import urllib.request
import urllib.parse
import re
#import sys
import requests
from urllib.request import urlparse,urljoin
from bs4 import BeautifulSoup
import colorama
import streamlit as st

st.title('Project X')
st.sidebar.title('Settings')

#st.markdown('<style>body{background-color: black;}</style>',unsafe_allow_html=True)
check_box_port=st.sidebar.checkbox(label='Port Scanner')
check_box_subs=st.sidebar.checkbox(label='Subdomain Scanner')
check_box_URL=st.sidebar.checkbox(label='URL Extractor')
if(check_box_port):
    st.title('Port Scanner')

    name = st.text_input("Enter URL")
    # print(type(name))
    remote_server = str(name)
    print(remote_server)

    if (st.button("Scan Ports")):
        remoteServerIP = socket.gethostbyname(remote_server)
        try:
            for port in range(1, 1000):
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                result = sock.connect_ex((remoteServerIP, port))
                if (result == 0):
                    print('{} is open'.format(port))
                    st.text("IP is {} and {} is open".format(remoteServerIP, port))

                # else:
                #   print('{} is Closed'.format(port))
                #  st.text('IP is {} and {} is closed'.format(remoteServerIP,port))

                sock.close()


        except KeyboardInterrupt:
            st.text("You pressed Ctrl+C")
            # sys.exit()

        except socket.gaierror:
            st.text('Hostname could not be resolved. Exiting')
            # sys.exit()

        except socket.error:
            st.text("Couldn't connect to server")
            # sys.exit()

if(check_box_subs):
    target = st.text_input('Enter a Domain Name')
    if (st.button("Scan Subdomains")):
        domains = set()
        # print((i, arg))
        with urllib.request.urlopen('https://crt.sh/?q=' + urllib.parse.quote('%.' + target)) as f:
            code = f.read().decode('utf-8')
            # print(code)
            for cert, domain in re.findall(
                    '<tr>(?:\s|\S)*?href="\?id=([0-9]+?)"(?:\s|\S)*?<td>([*_a-zA-Z0-9.-]+?\.' + re.escape(
                            target) + ')</td>(?:\s|\S)*?</tr>', code, re.IGNORECASE):
                # print((cert,domain))
                domain = domain.split('@')[-1]
                # print(domain)
                if not domain in domains:
                    domains.add(domain)
                    print(domain)

                    st.text('{}'.format(domain))

if(check_box_URL):
    st.title('URL Extractor')
    url = st.text_input("Enter URL")
    if(st.button("Extract URL")):

        colorama.init()

        GREEN = colorama.Fore.GREEN
        GRAY = colorama.Fore.LIGHTBLACK_EX
        RESET = colorama.Fore.RESET

        # initialize the set of links (unique links)
        internal_urls = set()
        external_urls = set()

        total_urls_visited = 0


        def is_valid(url):
            """
            Checks whether `url` is a valid URL.
            """
            parsed = urlparse(url)
            # print(parsed)
            return bool(parsed.netloc) and bool(parsed.scheme)


        def get_all_website_links(url):
            """
            Returns all URLs that is found on `url` in which it belongs to the same website
            """
            # all URLs of `url`
            urls = set()
            # domain name of the URL without the protocol
            domain_name = urlparse(url).netloc
            soup = BeautifulSoup(requests.get(url).content, "html.parser")
            for a_tag in soup.findAll("a"):
                href = a_tag.attrs.get("href")
                if href == "" or href is None:
                    # href empty tag
                    continue
                # join the URL if it's relative (not absolute link)
                href = urljoin(url, href)
                parsed_href = urlparse(href)
                # remove URL GET parameters, URL fragments, etc.
                href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path
                if not is_valid(href):
                    # not a valid URL
                    continue
                if href in internal_urls:
                    # already in the set
                    continue
                if domain_name not in href:
                    # external link
                    if href not in external_urls:
                        st.text(f"{GRAY}[!] External link: {href}{RESET}")
                        external_urls.add(href)
                    continue
                st.text(f"{GREEN}[*] Internal link: {href}{RESET}")
                urls.add(href)
                internal_urls.add(href)
            return urls


        def crawl(url, max_urls=50):
            """
            Crawls a web page and extracts all links.
            You'll find all links in `external_urls` and `internal_urls` global set variables.
            params:
                max_urls (int): number of max urls to crawl, default is 30.
            """
            global total_urls_visited
            total_urls_visited += 1
            links = get_all_website_links(url)
            for link in links:
                if total_urls_visited > max_urls:
                    break
                crawl(link, max_urls=max_urls)


        crawl(str(url))









